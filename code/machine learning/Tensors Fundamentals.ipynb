{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3143ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c3ab6",
   "metadata": {},
   "source": [
    "### **Essence of Tensors**\n",
    "\n",
    "Unlike list or tuple in python, tensors are stored as contiguous memory blocks. Each element in tensor by default is stored as 32-bit (4 Bytes) float.\n",
    "\n",
    "Instead of seperate memory blocks to store a set of variable. Tensors uses views to access a specific set of indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c5ad2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45a20f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = list(range(6))\n",
    "some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e333289a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[::2] # Start:End:Incrementor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7b224",
   "metadata": {},
   "source": [
    "### **Named Tensors**\n",
    "\n",
    "As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87349d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3,5,5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2f8ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) \n",
    "# shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85c4e1",
   "metadata": {},
   "source": [
    "So sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1. But we can generalize by counting from the end: they are always in dimension `â€“3`, the third from the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4360b9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0399, -0.0449, -0.4924,  0.7555, -0.6933],\n",
       "        [-0.0703,  0.2722, -0.5207,  0.2151, -0.0496],\n",
       "        [ 0.1879, -0.4988, -0.1784, -0.6670,  0.3736],\n",
       "        [-0.0618, -0.4370, -0.8501,  0.2922, -0.0737],\n",
       "        [ 0.6080, -0.1626, -0.0762, -1.0157,  0.1051]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d42b096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 5]), torch.Size([3]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t.mean(-3).shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3dd80cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_t * weights.unsqueeze(-1).unsqueeze(-1)).sum(-3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15655330",
   "metadata": {},
   "source": [
    "`unsqueeze()` Returns a new tensor with a dimension of size one inserted at the specified position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8f1fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 5, 5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.unsqueeze(-3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75780701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cef70e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.unsqueeze(-1).unsqueeze(-1).shape, batch_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1bf67",
   "metadata": {},
   "source": [
    "In a case to multiply the above two tensors, the multiplication would result in a tensor of shape `(2,3,5,5)` as `(3,1,1)` will be broadcasted to all the columns, rows and channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3f74ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted_fancy = torch.einsum(\n",
    "    '...chw,c->...hw', img_t, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "920e29ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f0e0b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-84-23d37e0242be>:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:934.)\n",
      "  weights_named = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "weights_named = torch.tensor(\n",
    "    [0.2126, 0.7152, 0.0722], names=['channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7cef3f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b822f3",
   "metadata": {},
   "source": [
    "When we already have a tensor and want to add names (but not change existing ones), we can call the method refine_names on it. Similar to indexing, the ellipsis `(...)` allows you to leave out any number of dimensions. With the rename sibling method, you can also overwrite or drop (by passing in None ) existing names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d9054d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., \n",
    "                               'channels', 'rows', 'columns')\n",
    "img_named.shape, img_named.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33379ae",
   "metadata": {},
   "source": [
    "The standard Python numeric types can be suboptimal for several reasons:\n",
    "\n",
    "1. Numbers in python are objects: When we store a floating-point number which is 32 bits, python converts it to python pbject with reference counting. This is known as boxing. ANd can be really inefficient to allocate millions of floating point numbers\n",
    "\n",
    "\n",
    "2. Lists in Python are meant for sequential collections of objects.\n",
    "\n",
    "\n",
    "3. The Python interpreter is slow compared to optimized, compiled code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a0993fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.5974802493302915e-43\n",
       " 7.705245871670389e+31\n",
       " 7.2147958764451125e+22\n",
       " 2.522590675557027e-18\n",
       " 2.5929804969848647e-09\n",
       " 1.0299355986120862e-11\n",
       " 7.720500350139048e-10\n",
       " 1.0503973498998675e-05\n",
       " 4.0518900734642926e-11\n",
       " 6.712973004141531e-07\n",
       " 2.9571086513311325e-18\n",
       "[torch.FloatStorage of size 12]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Storage(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ab375",
   "metadata": {},
   "source": [
    "### Tensor Storage: Under the hood\n",
    "Values in tensors are allocated in contiguous chunks of memory managed by `torch.Storage` instances. A storage is a one -dimensional array of numerical data: that is, a contiguous\n",
    "block of memory containing numbers of a given type, such as float (32 bits representing a floating-point number) or int64 (64 bits representing an integer).\n",
    "\n",
    "The underlying memory is **allocated only once**, managed by the Storage instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c65e1e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.storage()\n",
    "\n",
    "# The tensor just knows how to translate a pair of indices \n",
    "# into a location in the storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf62851",
   "metadata": {},
   "source": [
    "### Tensor metadata: Size, offset, and stride\n",
    "\n",
    "1. Size: Indicating how many elements across each dimension the tensor represents.\n",
    "\n",
    "\n",
    "2. Offset: Offset is the index in the storage corresponding to the first element in the tensor.\n",
    "\n",
    "\n",
    "3. Stride: Stride is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52d4dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point = points[1]\n",
    "print(second_point)\n",
    "second_point.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6b225c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.size(), second_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "12f69770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be2503f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[2].storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14198f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(4,3).stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c6f00",
   "metadata": {},
   "source": [
    "Accessing an element `i, j` in a 2D tensor results in accessing the `storage_offset + stride[0] * i + stride[1] * j` element in the storage.\n",
    "\n",
    "### Contiguous tensors\n",
    "\n",
    "Itâ€™s worth noting that calling `contiguous` will do nothing (and will not hurt performance) if the tensor is already contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7edb4f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 5., 2.],\n",
       "         [1., 3., 1.]]),\n",
       "  4.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.FloatStorage of size 6],\n",
       "  4.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.FloatStorage of size 6])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t = points.T\n",
    "points_t, points.storage(), points_t.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d64b6539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, (1, 2))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.is_contiguous(), points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0952918f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont = points_t.contiguous()\n",
    "points_t_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c41e064a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( 4.0\n",
       "  5.0\n",
       "  2.0\n",
       "  1.0\n",
       "  3.0\n",
       "  1.0\n",
       " [torch.FloatStorage of size 6],\n",
       " (3, 1))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont.storage(), points_t_cont.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349f176",
   "metadata": {},
   "source": [
    "HDF5 is a portable, widely supported format for representing serialized multidimensional arrays, organized in a nested key-value dictionary. Python supports HDF5 through the h5py library (www.h5py.org), which accepts and returns data in the form of NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7893bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('points.hdf5', 'w')\n",
    "dset = f.create_dataset('coords', data=points.numpy())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "101bf261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"coords\": shape (3, 2), type \"<f4\">"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('points.hdf5', 'r')\n",
    "dset = f['coords']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fe880c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3.],\n",
       "       [2., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26d6b5",
   "metadata": {},
   "source": [
    "The data is not loaded when the file is opened or the dataset is required. Rather, the data stays on disk until we request the second and last rows in the dataset. At that point, h5py accesses those two columns and returns a NumPy array-like object encapsulating that region in that dataset that behaves like a NumPy array and has the same API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26530ef",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "246aac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(list(range(9)), dtype=torch.float)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "77e35687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, torch.Size([9]), (1,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), a.size(), a.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f86448ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bd9bda69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( 0.0\n",
       "  1.0\n",
       "  2.0\n",
       "  3.0\n",
       "  4.0\n",
       "  5.0\n",
       "  6.0\n",
       "  7.0\n",
       "  8.0\n",
       " [torch.FloatStorage of size 9],\n",
       "  0.0\n",
       "  1.0\n",
       "  2.0\n",
       "  3.0\n",
       "  4.0\n",
       "  5.0\n",
       "  6.0\n",
       "  7.0\n",
       "  8.0\n",
       " [torch.FloatStorage of size 9])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.storage(), a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "70995c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), 4, (3, 1))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b[1:, 1:]\n",
    "# size = [2,2] ; offset = 4 ; stride = (2,1)\n",
    "c.size(), c.storage_offset(), c.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "14949d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "         [-0.9900, -0.6536,  0.2837],\n",
       "         [ 0.9602,  0.7539, -0.1455]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(b), b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "beb3422f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.unsqueeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "db799fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f58d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
